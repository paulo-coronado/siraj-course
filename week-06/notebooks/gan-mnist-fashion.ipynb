{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"Using TensorFlow backend.\n"}],"source":"import os\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.datasets import fashion_mnist"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"root = os.getcwd()\nmodels = os.path.join(root,'models')\nutils = os.path.join(root,'utils')\nrun = os.path.join(root,'run')\n\nsys.path.append(root)\nsys.path.append(models)\nsys.path.append(utils)"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"from models.GAN import GAN"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"# run params\nSECTION = 'gan'\nRUN_ID = '0001'\nDATA_NAME = 'mnist_fashion'\nRUN_FOLDER = os.path.join(run, SECTION, '_'.join([RUN_ID, DATA_NAME]))\n\n\nif not os.path.exists(RUN_FOLDER):\n    os.mkdir(RUN_FOLDER)\n    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"mode =  'build' #'load' #"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"def load_mnist_fashion():\n    # Load the dataset\n    (X_train, _), (_, _) = fashion_mnist.load_data()\n    \n    return X_train\n\nx_train = load_mnist_fashion()"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":"(60000, 28, 28)"},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":"x_train.shape"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":"<matplotlib.image.AxesImage at 0x27f6cd9ff98>"},"execution_count":8,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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\n","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 251.565 248.518125 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\nL 244.365 7.2 \r\nL 26.925 7.2 \r\nz\r\n\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p2bc7c874f2)\">\r\n    <image height=\"218\" id=\"image9cfa6e7421\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAABHNCSVQICAgIfAhkiAAAChVJREFUeJzt3U9vTW0fxfHdm7boqaLSokXaICISAxETBgY1IBITMZaIgbfgFTAy9Q4MTcwYEJEgTYSBaBH/HWm11J+WUvfIM9trPezsderO9zNduU7P2ceyk/PLde22oih+FjVpa2uT+a5du2R+8OBBmZ87d+6339Mv7r39/FnbZflPq/O6njlzRuajo6Myv3btmsxnZ2d/+z39v/6p7ZUB/A9FAwIoGhBA0YAAigYEUDQggKIBAUvrfPFt27bJ/MCBAzLfv3+/zC9cuFCaffjwQa5ljrb4NBoNmff398t8aGhI5mNjYzJ/8uSJzKvgjgYEUDQggKIBARQNCKBoQABFAwIoGhBQ6xytp6dH5m/evJH5zMyMzNVcxc3RFhYWZI4/U2X+uHbtWpm77/Tz588y7+vrkzlzNOAvR9GAAIoGBFA0IICiAQEUDQio9ef9qakpmX/58kXmR44ckfnw8HBp5rZEYPF5+vRppfUvXryQ+cTERKXXr4I7GhBA0YAAigYEUDQggKIBARQNCKBoQECtc7SlS/XLL1myROadnZ0y37JlS2nmjpM7evSozJ01a9bI/J9/yv8Pm5+fl2s7Ojpk/uPHD5m7LUDq73///r3S33bf2ZUrV0qzZrMp17ptV26bzfj4uMzrxB0NCKBoQABFAwIoGhBA0YAAigYEUDQgoNY5mjt6zM263Dxo8+bNf/y3d+7cKfOXL1/K3FGfbfny5XJt1aPw3BxOvb6bfTrus3379q006+rqkmvdcXFuf6P791Yn7mhAAEUDAigaEEDRgACKBgRQNCCAogEBtc7R5ubmZF51jqb2fDkPHjyQ+bt372Su5kFF4WdZdXLzKLUfzX0nbk7mHp2k5nRuTua+k+npaZm7vXZ14o4GBFA0IICiAQEUDQigaEAARQMCKBoQUOsczZ0B6PaMufMPe3t7f/s9/eKexVX1zEn12T5+/CjXtre3y9zND93rq3mSm8FNTk7K3M0P1Xt336d7b+66uO+0TtzRgACKBgRQNCCAogEBFA0IoGhAQEsf2+S2XLif/zdu3Pjb7+kX91Nvd3e3zN02GfUTetWtKO66up+5Ve5+nndbTdx6tZXFfd9Vzc7O1vr6Cnc0IICiAQEUDQigaEAARQMCKBoQQNGAgFrnaO4xOhs2bJD5+/fvZb5y5crSzM2iqm5VcXM0NStzs6avX7/W9reLQs/R3BGB7rosW7ZM5uqz7dq1S651x8n19PTIvO45ncIdDQigaEAARQMCKBoQQNGAAIoGBFA0IKDWOVrVvUvuuLnHjx+XZu6RT24W5bh5knp8UaPRkGvddav6OCt1Xd0Rge47c8fRKadPn5b5xYsXZe6ui/tsdeKOBgRQNCCAogEBFA0IoGhAAEUDAigaEFDrHE3NkorCP4bHzT3U2Yw7d+6Ua91jm9yZkVXnTYqbg9XJnRnp9rrNzMz88d++efOmzN180e2layXuaEAARQMCKBoQQNGAAIoGBFA0IKDWn/fdNhf3aKTx8XGZDw4Olmb79u2Ta0dHR2Xu3pvbDqJ+Bq96XJzL3XYRlXd2dsq17sg2d93Ud+aO2XNjD3ddq26NqoI7GhBA0YAAigYEUDQggKIBARQNCKBoQECtczQ3z3HzoNnZWZmrxzr19fXJtY57rJN772pLh9uK4vKqx6apWZjbiuLmaO69HT9+vDRz21xcrrZNFYX/bHXijgYEUDQggKIBARQNCKBoQABFAwIoGhBQ6xzNmZiYkPnAwIDM1Vzl4cOHcu26detk7vZGuRmhmum4WZTLHTfjU3vO3BGBW7dulfmdO3dkrr4X976rzvDc/sg6cUcDAigaEEDRgACKBgRQNCCAogEBFA0IaOkcze0PcmcEqlnXo0eP5Npmsynz7du3y/zTp08yV3vK3DzIXRc3w3PUe5uampJr3Rzt0qVLMj916lRp5h7b5Gaf7e3tMm8l7mhAAEUDAigaEEDRgACKBgRQNCCAogEBLZ2juVmU2xO2Zs2a0mz//v1yrZvZLFu2TObu+WhqVtXR0SHXun1VLnfnQqrr6p5B5rgZ4MmTJ0uzW7duVXptdc5nq3FHAwIoGhBA0YAAigYEUDQggKIBAS39ed89hsf9XNtoNEqzw4cPy7Vnz56V+fLly2XuqJ/Yv337Vum1HTc+UEfKDQ0NybVuG83evXtlrh6n5cY5bnuRGxe1Enc0IICiAQEUDQigaEAARQMCKBoQQNGAgJbO0WZnZ2Xutnu8evWqNLt///4fvadf3FYUt42mymOb3N9220XcejVvGh4elmvHxsZk/vHjR5mrxzpV3ebi/j21Enc0IICiAQEUDQigaEAARQMCKBoQQNGAgJbO0dzMxc2b7t27V5odPHhQrh0ZGZH569evZe7maOqzuccLrVixQuZuP5ubo3V2dpZm7nM9e/ZM5seOHZO5OlLO7QFU++iKwu9vbCXuaEAARQMCKBoQQNGAAIoGBFA0IICiAQEtnaPNz8/LvMq+rMePH8u158+fl7l77NOWLVtkrmZl7tFI7969k7mbN7k5nJpPjo+Py7XuO3FnMzabzdLM7T90/14+fPgg81bijgYEUDQggKIBARQNCKBoQABFAwIoGhDQVhSF3vTVQm7PmJrZjI6OyrV79uyRudsr5+ZNa9euLc2qnmfp5klujqa4WVRPT4/MN23aJPOBgYHS7O3bt3Kt24d3/fp1mbcSdzQggKIBARQNCKBoQABFAwIoGhDQ0m0yjvsZXG33GBwclGufP38u8y9fvsjcbReZnJyUueJ+Qnc/77ttNkpHR4fM1SOfisKPRdR1cdd0ampK5osZdzQggKIBARQNCKBoQABFAwIoGhBA0YCART1Hc9si1JFubW1tcq3axlIUfruIe/RSFVUf6+SOs1MzQvVIp6Lwj9JyszA142s0GnKtm+EtZtzRgACKBgRQNCCAogEBFA0IoGhAAEUDAhb1HM3tP+rv7y/NFhYW5NqZmRmZu3lQd3e3zBU3H3T78Nyczc3CVq9eLfMq3GdTuZsPfv78+Y/e02LAHQ0IoGhAAEUDAigaEEDRgACKBgRQNCBgUc/RXr16JfOhoaHSzO3Jcvuq3PmGbsbXbDZLs/Xr18u1fX19Mp+bm5O5u25qvTsP05056a77qlWrSjM3u5yYmJD5YsYdDQigaEAARQMCKBoQQNGAAIoGBCzqn/fddpEnT56UZgMDA3Kt2yYzPT0t8927d8v8xIkTpdnw8LBce/XqVZm70cLIyIjMb9++XZpdvHhRrnWPZert7ZV5V1dXaXbjxg259m/GHQ0IoGhAAEUDAigaEEDRgACKBgRQNCCgrSgKvV/kL+XmOe4RQdu3b5f5oUOHZP769evS7O7du3Ktm+G5rSru2LYdO3aUZm6by+XLl2U+OTkp85cvX8r8v4o7GhBA0YAAigYEUDQggKIBARQNCKBoQMC/4a8qOvgFPl0AAAAASUVORK5CYII=\" y=\"-6.64\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"me35283ba48\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#me35283ba48\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#me35283ba48\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#me35283ba48\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#me35283ba48\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#me35283ba48\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#me35283ba48\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m2a7a5ef1be\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m2a7a5ef1be\" y=\"11.082857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m2a7a5ef1be\" y=\"49.911429\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m2a7a5ef1be\" y=\"88.74\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m2a7a5ef1be\" y=\"127.568571\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m2a7a5ef1be\" y=\"166.397143\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m2a7a5ef1be\" y=\"205.225714\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 224.64 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p2bc7c874f2\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{},"output_type":"display_data"}],"source":"plt.imshow(x_train[400,:,:],cmap='gray')"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"gan = GAN(input_dim = (28, 28, 1)\n        , discriminator_conv_filters = [64, 64, 128, 128]\n        , discriminator_conv_kernel_size = [5, 5, 5, 5]\n        , discriminator_conv_strides = [2, 2, 2, 1]\n        , discriminator_batch_norm_momentum = .8\n        , discriminator_activation = 'relu'\n        , discriminator_dropout_rate = 0.4\n        , discriminator_learning_rate = 0.0002\n        , generator_initial_dense_layer_size = (7, 7, 64)\n        , generator_upsample = [2, 2, 1, 1]\n        , generator_conv_filters = [128, 64, 64, 1]\n        , generator_conv_kernel_size = [5, 5, 5, 5]\n        , generator_conv_strides = [1, 1, 1, 1]\n        , generator_batch_norm_momentum = 0.8\n        , generator_activation = 'relu'\n        , generator_dropout_rate = None\n        , generator_learning_rate = 0.0004\n        , optimiser = 'adam'\n        , z_dim = 100\n        )"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndiscriminator_input (InputLa (None, 28, 28, 1)         0         \n_________________________________________________________________\ndiscriminator_conv_0 (Conv2D (None, 14, 14, 64)        1664      \n_________________________________________________________________\nactivation_1 (Activation)    (None, 14, 14, 64)        0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 14, 14, 64)        0         \n_________________________________________________________________\ndiscriminator_conv_1 (Conv2D (None, 7, 7, 64)          102464    \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 7, 7, 64)          256       \n_________________________________________________________________\nactivation_2 (Activation)    (None, 7, 7, 64)          0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 7, 7, 64)          0         \n_________________________________________________________________\ndiscriminator_conv_2 (Conv2D (None, 4, 4, 128)         204928    \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 4, 4, 128)         512       \n_________________________________________________________________\nactivation_3 (Activation)    (None, 4, 4, 128)         0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 4, 4, 128)         0         \n_________________________________________________________________\ndiscriminator_conv_3 (Conv2D (None, 4, 4, 128)         409728    \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 4, 4, 128)         512       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 4, 4, 128)         0         \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 4, 4, 128)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 2048)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 2049      \n=================================================================\nTotal params: 722,113\nTrainable params: 721,473\nNon-trainable params: 640\n_________________________________________________________________\n"}],"source":"gan.discriminator.summary()"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Model: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ngenerator_input (InputLayer) (None, 100)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 3136)              316736    \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 3136)              12544     \n_________________________________________________________________\nactivation_5 (Activation)    (None, 3136)              0         \n_________________________________________________________________\nreshape_1 (Reshape)          (None, 7, 7, 64)          0         \n_________________________________________________________________\nup_sampling2d_1 (UpSampling2 (None, 14, 14, 64)        0         \n_________________________________________________________________\ngenerator_conv_0 (Conv2D)    (None, 14, 14, 128)       204928    \n_________________________________________________________________\nbatch_normalization_5 (Batch (None, 14, 14, 128)       512       \n_________________________________________________________________\nactivation_6 (Activation)    (None, 14, 14, 128)       0         \n_________________________________________________________________\nup_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n_________________________________________________________________\ngenerator_conv_1 (Conv2D)    (None, 28, 28, 64)        204864    \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 28, 28, 64)        256       \n_________________________________________________________________\nactivation_7 (Activation)    (None, 28, 28, 64)        0         \n_________________________________________________________________\ngenerator_conv_2 (Conv2DTran (None, 28, 28, 64)        102464    \n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 28, 28, 64)        256       \n_________________________________________________________________\nactivation_8 (Activation)    (None, 28, 28, 64)        0         \n_________________________________________________________________\ngenerator_conv_3 (Conv2DTran (None, 28, 28, 1)         1601      \n_________________________________________________________________\nactivation_9 (Activation)    (None, 28, 28, 1)         0         \n=================================================================\nTotal params: 844,161\nTrainable params: 837,377\nNon-trainable params: 6,784\n_________________________________________________________________\n"}],"source":"gan.generator.summary()"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"BATCH_SIZE = 64\nEPOCHS = 6000\nPRINT_EVERY_N_BATCHES = 5"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"C:\\Users\\benmc\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n  'Discrepancy between trainable weights and collected trainable'\n0 [D loss: (2.106)(R 0.447, F 3.765)] [D acc: (0.445)(0.891, 0.000)] [G loss: 0.433] [G acc: 0.812]\nC:\\Users\\benmc\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n  'Discrepancy between trainable weights and collected trainable'\n1 [D loss: (1.068)(R 0.324, F 1.813)] [D acc: (0.484)(0.969, 0.000)] [G loss: 1.026] [G acc: 0.234]\n2 [D loss: (0.736)(R 0.515, F 0.956)] [D acc: (0.594)(0.844, 0.344)] [G loss: 1.160] [G acc: 0.219]\n3 [D loss: (1.091)(R 0.580, F 1.601)] [D acc: (0.391)(0.750, 0.031)] [G loss: 1.197] [G acc: 0.141]\n4 [D loss: (0.910)(R 0.748, F 1.072)] [D acc: (0.328)(0.453, 0.203)] [G loss: 1.282] [G acc: 0.141]\n5 [D loss: (1.001)(R 0.941, F 1.061)] [D acc: (0.312)(0.391, 0.234)] [G loss: 1.284] [G acc: 0.156]\n6 [D loss: (0.699)(R 0.969, F 0.429)] [D acc: (0.594)(0.344, 0.844)] [G loss: 0.765] [G acc: 0.531]\n7 [D loss: (0.657)(R 0.627, F 0.687)] [D acc: (0.664)(0.719, 0.609)] [G loss: 0.797] [G acc: 0.406]\n8 [D loss: (0.509)(R 0.661, F 0.356)] [D acc: (0.758)(0.578, 0.938)] [G loss: 0.581] [G acc: 0.672]\n9 [D loss: (0.500)(R 0.461, F 0.538)] [D acc: (0.797)(0.828, 0.766)] [G loss: 0.567] [G acc: 0.688]\n10 [D loss: (0.486)(R 0.456, F 0.515)] [D acc: (0.805)(0.812, 0.797)] [G loss: 0.546] [G acc: 0.703]\n11 [D loss: (0.555)(R 0.422, F 0.689)] [D acc: (0.742)(0.828, 0.656)] [G loss: 0.769] [G acc: 0.469]\n12 [D loss: (0.507)(R 0.523, F 0.491)] [D acc: (0.797)(0.766, 0.828)] [G loss: 0.897] [G acc: 0.344]\n13 [D loss: (0.948)(R 0.430, F 1.466)] [D acc: (0.508)(0.844, 0.172)] [G loss: 1.137] [G acc: 0.250]\n14 [D loss: (1.110)(R 0.766, F 1.454)] [D acc: (0.273)(0.453, 0.094)] [G loss: 1.533] [G acc: 0.031]\n15 [D loss: (0.939)(R 1.144, F 0.733)] [D acc: (0.398)(0.234, 0.562)] [G loss: 1.373] [G acc: 0.188]\n16 [D loss: (0.932)(R 0.872, F 0.993)] [D acc: (0.398)(0.469, 0.328)] [G loss: 1.421] [G acc: 0.031]\n17 [D loss: (0.914)(R 1.021, F 0.806)] [D acc: (0.367)(0.234, 0.500)] [G loss: 1.399] [G acc: 0.156]\n18 [D loss: (0.945)(R 0.956, F 0.935)] [D acc: (0.336)(0.359, 0.312)] [G loss: 1.479] [G acc: 0.109]\n19 [D loss: (0.979)(R 1.147, F 0.810)] [D acc: (0.352)(0.312, 0.391)] [G loss: 1.386] [G acc: 0.109]\n20 [D loss: (1.039)(R 1.130, F 0.948)] [D acc: (0.289)(0.234, 0.344)] [G loss: 1.411] [G acc: 0.062]\n21 [D loss: (0.875)(R 1.014, F 0.736)] [D acc: (0.422)(0.281, 0.562)] [G loss: 1.308] [G acc: 0.172]\n22 [D loss: (0.724)(R 0.827, F 0.621)] [D acc: (0.539)(0.438, 0.641)] [G loss: 1.398] [G acc: 0.062]\n23 [D loss: (0.777)(R 0.865, F 0.689)] [D acc: (0.539)(0.484, 0.594)] [G loss: 1.283] [G acc: 0.156]\n24 [D loss: (0.913)(R 1.114, F 0.712)] [D acc: (0.422)(0.297, 0.547)] [G loss: 1.140] [G acc: 0.203]\n25 [D loss: (0.780)(R 0.797, F 0.762)] [D acc: (0.477)(0.500, 0.453)] [G loss: 1.285] [G acc: 0.156]\n26 [D loss: (0.779)(R 0.931, F 0.627)] [D acc: (0.516)(0.391, 0.641)] [G loss: 1.302] [G acc: 0.109]\n27 [D loss: (0.713)(R 0.855, F 0.572)] [D acc: (0.531)(0.406, 0.656)] [G loss: 1.194] [G acc: 0.234]\n28 [D loss: (0.722)(R 0.840, F 0.605)] [D acc: (0.555)(0.453, 0.656)] [G loss: 0.809] [G acc: 0.406]\n29 [D loss: (0.608)(R 0.589, F 0.628)] [D acc: (0.695)(0.703, 0.688)] [G loss: 0.870] [G acc: 0.453]\n30 [D loss: (0.759)(R 0.570, F 0.947)] [D acc: (0.539)(0.672, 0.406)] [G loss: 1.277] [G acc: 0.078]\n31 [D loss: (0.681)(R 0.862, F 0.500)] [D acc: (0.594)(0.438, 0.750)] [G loss: 1.085] [G acc: 0.188]\n32 [D loss: (0.771)(R 0.813, F 0.729)] [D acc: (0.445)(0.469, 0.422)] [G loss: 1.078] [G acc: 0.266]\n33 [D loss: (0.659)(R 0.836, F 0.483)] [D acc: (0.617)(0.484, 0.750)] [G loss: 1.123] [G acc: 0.266]\n34 [D loss: (0.794)(R 0.680, F 0.908)] [D acc: (0.477)(0.562, 0.391)] [G loss: 1.447] [G acc: 0.156]\n35 [D loss: (0.588)(R 0.697, F 0.478)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.372] [G acc: 0.125]\n36 [D loss: (0.684)(R 0.641, F 0.727)] [D acc: (0.602)(0.609, 0.594)] [G loss: 1.267] [G acc: 0.234]\n37 [D loss: (0.791)(R 0.792, F 0.790)] [D acc: (0.469)(0.500, 0.438)] [G loss: 1.351] [G acc: 0.141]\n38 [D loss: (0.699)(R 0.772, F 0.625)] [D acc: (0.609)(0.562, 0.656)] [G loss: 1.380] [G acc: 0.125]\n39 [D loss: (0.736)(R 0.839, F 0.634)] [D acc: (0.578)(0.500, 0.656)] [G loss: 1.458] [G acc: 0.125]\n40 [D loss: (0.634)(R 0.631, F 0.637)] [D acc: (0.656)(0.688, 0.625)] [G loss: 1.455] [G acc: 0.016]\n41 [D loss: (0.557)(R 0.578, F 0.536)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.319] [G acc: 0.109]\n42 [D loss: (0.557)(R 0.648, F 0.467)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.304] [G acc: 0.094]\n43 [D loss: (0.392)(R 0.488, F 0.296)] [D acc: (0.891)(0.828, 0.953)] [G loss: 1.201] [G acc: 0.219]\n44 [D loss: (0.577)(R 0.525, F 0.628)] [D acc: (0.695)(0.750, 0.641)] [G loss: 1.627] [G acc: 0.078]\n45 [D loss: (0.648)(R 0.699, F 0.596)] [D acc: (0.609)(0.562, 0.656)] [G loss: 1.696] [G acc: 0.047]\n46 [D loss: (0.622)(R 0.684, F 0.560)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.959] [G acc: 0.031]\n47 [D loss: (0.582)(R 0.688, F 0.476)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.824] [G acc: 0.062]\n48 [D loss: (0.604)(R 0.696, F 0.513)] [D acc: (0.680)(0.609, 0.750)] [G loss: 2.226] [G acc: 0.000]\n49 [D loss: (0.533)(R 0.611, F 0.455)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.963] [G acc: 0.000]\n50 [D loss: (0.519)(R 0.445, F 0.593)] [D acc: (0.750)(0.797, 0.703)] [G loss: 2.000] [G acc: 0.031]\n51 [D loss: (0.460)(R 0.471, F 0.449)] [D acc: (0.773)(0.766, 0.781)] [G loss: 2.308] [G acc: 0.016]\n52 [D loss: (0.389)(R 0.412, F 0.366)] [D acc: (0.891)(0.844, 0.938)] [G loss: 2.403] [G acc: 0.016]\n53 [D loss: (0.446)(R 0.485, F 0.407)] [D acc: (0.773)(0.719, 0.828)] [G loss: 2.695] [G acc: 0.000]\n54 [D loss: (0.343)(R 0.469, F 0.217)] [D acc: (0.875)(0.781, 0.969)] [G loss: 2.645] [G acc: 0.000]\n55 [D loss: (0.217)(R 0.241, F 0.192)] [D acc: (0.953)(0.953, 0.953)] [G loss: 2.476] [G acc: 0.000]\n56 [D loss: (0.234)(R 0.286, F 0.183)] [D acc: (0.945)(0.906, 0.984)] [G loss: 2.284] [G acc: 0.000]\n57 [D loss: (0.217)(R 0.220, F 0.214)] [D acc: (0.945)(0.938, 0.953)] [G loss: 2.118] [G acc: 0.031]\n58 [D loss: (0.249)(R 0.189, F 0.310)] [D acc: (0.953)(0.969, 0.938)] [G loss: 3.328] [G acc: 0.000]\n59 [D loss: (0.296)(R 0.378, F 0.213)] [D acc: (0.883)(0.844, 0.922)] [G loss: 3.026] [G acc: 0.000]\n60 [D loss: (0.237)(R 0.245, F 0.228)] [D acc: (0.938)(0.922, 0.953)] [G loss: 3.409] [G acc: 0.000]\n61 [D loss: (0.186)(R 0.219, F 0.152)] [D acc: (0.945)(0.922, 0.969)] [G loss: 3.255] [G acc: 0.000]\n62 [D loss: (0.161)(R 0.207, F 0.115)] [D acc: (0.977)(0.953, 1.000)] [G loss: 3.169] [G acc: 0.000]\n63 [D loss: (0.180)(R 0.196, F 0.164)] [D acc: (0.969)(0.938, 1.000)] [G loss: 3.223] [G acc: 0.000]\n64 [D loss: (0.097)(R 0.108, F 0.085)] [D acc: (1.000)(1.000, 1.000)] [G loss: 3.274] [G acc: 0.000]\n65 [D loss: (0.195)(R 0.201, F 0.189)] [D acc: (0.938)(0.922, 0.953)] [G loss: 3.817] [G acc: 0.000]\n66 [D loss: (0.108)(R 0.156, F 0.061)] [D acc: (0.984)(0.969, 1.000)] [G loss: 3.576] [G acc: 0.000]\n67 [D loss: (0.079)(R 0.056, F 0.102)] [D acc: (1.000)(1.000, 1.000)] [G loss: 3.977] [G acc: 0.000]\n68 [D loss: (0.090)(R 0.115, F 0.064)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.194] [G acc: 0.000]\n69 [D loss: (0.066)(R 0.070, F 0.062)] [D acc: (0.992)(1.000, 0.984)] [G loss: 4.440] [G acc: 0.000]\n70 [D loss: (0.050)(R 0.054, F 0.046)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.549] [G acc: 0.000]\n71 [D loss: (0.078)(R 0.092, F 0.064)] [D acc: (0.984)(0.969, 1.000)] [G loss: 4.188] [G acc: 0.000]\n72 [D loss: (0.064)(R 0.063, F 0.064)] [D acc: (1.000)(1.000, 1.000)] [G loss: 3.783] [G acc: 0.000]\n73 [D loss: (0.032)(R 0.025, F 0.040)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.399] [G acc: 0.000]\n74 [D loss: (0.048)(R 0.030, F 0.067)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.490] [G acc: 0.000]\n75 [D loss: (0.056)(R 0.087, F 0.024)] [D acc: (0.992)(0.984, 1.000)] [G loss: 4.997] [G acc: 0.000]\n76 [D loss: (0.050)(R 0.028, F 0.071)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.036] [G acc: 0.000]\n77 [D loss: (0.066)(R 0.098, F 0.033)] [D acc: (0.977)(0.953, 1.000)] [G loss: 5.355] [G acc: 0.000]\n78 [D loss: (0.024)(R 0.028, F 0.021)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.529] [G acc: 0.000]\n79 [D loss: (0.027)(R 0.028, F 0.027)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.870] [G acc: 0.000]\n80 [D loss: (0.021)(R 0.031, F 0.011)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.074] [G acc: 0.000]\n81 [D loss: (0.014)(R 0.010, F 0.019)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.756] [G acc: 0.000]\n82 [D loss: (0.054)(R 0.065, F 0.043)] [D acc: (0.992)(0.984, 1.000)] [G loss: 6.472] [G acc: 0.000]\n83 [D loss: (0.008)(R 0.011, F 0.005)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.033] [G acc: 0.000]\n84 [D loss: (0.020)(R 0.031, F 0.009)] [D acc: (0.992)(0.984, 1.000)] [G loss: 5.684] [G acc: 0.000]\n85 [D loss: (0.022)(R 0.016, F 0.027)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.152] [G acc: 0.000]\n86 [D loss: (0.007)(R 0.012, F 0.003)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.634] [G acc: 0.000]\n87 [D loss: (0.010)(R 0.012, F 0.007)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.441] [G acc: 0.000]\n88 [D loss: (0.004)(R 0.006, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.611] [G acc: 0.000]\n89 [D loss: (0.010)(R 0.008, F 0.012)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.334] [G acc: 0.000]\n90 [D loss: (0.009)(R 0.010, F 0.009)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.378] [G acc: 0.000]\n91 [D loss: (0.013)(R 0.012, F 0.014)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.981] [G acc: 0.000]\n92 [D loss: (0.012)(R 0.020, F 0.003)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.388] [G acc: 0.000]\n93 [D loss: (0.005)(R 0.003, F 0.007)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.856] [G acc: 0.000]\n94 [D loss: (0.004)(R 0.004, F 0.004)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.281] [G acc: 0.000]\n95 [D loss: (0.007)(R 0.011, F 0.003)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.871] [G acc: 0.000]\n96 [D loss: (0.003)(R 0.002, F 0.003)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.307] [G acc: 0.000]\n97 [D loss: (0.005)(R 0.003, F 0.007)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.171] [G acc: 0.000]\n98 [D loss: (0.006)(R 0.009, F 0.004)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.647] [G acc: 0.000]\n99 [D loss: (0.003)(R 0.001, F 0.004)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.651] [G acc: 0.000]\n100 [D loss: (0.007)(R 0.008, F 0.005)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.138] [G acc: 0.000]\n101 [D loss: (0.002)(R 0.001, F 0.003)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.701] [G acc: 0.000]\n102 [D loss: (0.006)(R 0.007, F 0.005)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.111] [G acc: 0.000]\n103 [D loss: (0.011)(R 0.014, F 0.008)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.563] [G acc: 0.000]\n104 [D loss: (0.002)(R 0.003, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.646] [G acc: 0.000]\n105 [D loss: (0.001)(R 0.001, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.568] [G acc: 0.000]\n106 [D loss: (0.002)(R 0.002, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.133] [G acc: 0.000]\n107 [D loss: (0.002)(R 0.003, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.155] [G acc: 0.000]\n108 [D loss: (0.002)(R 0.001, F 0.004)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.608] [G acc: 0.000]\n109 [D loss: (0.002)(R 0.003, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.674] [G acc: 0.000]\n110 [D loss: (0.001)(R 0.000, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.948] [G acc: 0.000]\n111 [D loss: (0.004)(R 0.006, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.665] [G acc: 0.000]\n112 [D loss: (0.001)(R 0.001, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.552] [G acc: 0.000]\n113 [D loss: (0.002)(R 0.002, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.122] [G acc: 0.000]\n114 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.075] [G acc: 0.000]\n115 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.351] [G acc: 0.000]\n116 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.553] [G acc: 0.000]\n117 [D loss: (0.001)(R 0.001, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.852] [G acc: 0.000]\n118 [D loss: (0.001)(R 0.001, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.517] [G acc: 0.000]\n119 [D loss: (0.001)(R 0.001, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.150] [G acc: 0.000]\n120 [D loss: (0.001)(R 0.001, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.564] [G acc: 0.000]\n121 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.501] [G acc: 0.000]\n122 [D loss: (0.000)(R 0.000, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.134] [G acc: 0.000]\n123 [D loss: (0.000)(R 0.001, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.684] [G acc: 0.000]\n124 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.040] [G acc: 0.000]\n125 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.248] [G acc: 0.000]\n126 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.807] [G acc: 0.000]\n127 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.382] [G acc: 0.000]\n128 [D loss: (0.001)(R 0.001, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.742] [G acc: 0.000]\n129 [D loss: (0.001)(R 0.001, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.676] [G acc: 0.000]\n130 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.802] [G acc: 0.000]\n131 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.168] [G acc: 0.000]\n132 [D loss: (0.000)(R 0.001, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.857] [G acc: 0.000]\n133 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.859] [G acc: 0.000]\n134 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.031] [G acc: 0.000]\n135 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.728] [G acc: 0.000]\n136 [D loss: (0.001)(R 0.001, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.108] [G acc: 0.000]\n137 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.464] [G acc: 0.000]\n138 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.702] [G acc: 0.000]\n139 [D loss: (0.000)(R 0.000, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.604] [G acc: 0.000]\n"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-14-9bcf059cbed1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;33m,\u001b[0m \u001b[0mrun_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRUN_FOLDER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;33m,\u001b[0m \u001b[0mprint_every_n_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPRINT_EVERY_N_BATCHES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m )\n","\u001b[1;32mc:\\projects\\ml-gan-fashion\\models\\GAN.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x_train, batch_size, epochs, run_folder, print_every_n_batches, using_generator)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musing_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"%d [D loss: (%.3f)(R %.3f, F %.3f)] [D acc: (%.3f)(%.3f, %.3f)] [G loss: %.3f] [G acc: %.3f]\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\projects\\ml-gan-fashion\\models\\GAN.py\u001b[0m in \u001b[0;36mtrain_generator\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":"x_train = np.expand_dims(x_train, axis=3)\n\ngan.train(     \n    x_train\n    , batch_size = BATCH_SIZE\n    , epochs = EPOCHS\n    , run_folder = RUN_FOLDER\n    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}